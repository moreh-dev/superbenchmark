diff --git a/superbench/benchmarks/model_benchmarks/model_base.py b/superbench/benchmarks/model_benchmarks/model_base.py
index 133ee76..a8a01ca 100644
--- a/superbench/benchmarks/model_benchmarks/model_base.py
+++ b/superbench/benchmarks/model_benchmarks/model_base.py
@@ -411,6 +411,8 @@ def __process_model_result(self, model_action, precision, step_times):
         # The unit of step time is millisecond, use it to calculate the throughput with the unit samples/sec.
         millisecond_per_second = 1000
         throughput = [millisecond_per_second / step_time * self._args.batch_size for step_time in step_times]
+        mean_throughput = millisecond_per_second / statistics.mean(step_times) * self._args.batch_size
+
         self._result.add_raw_data(metric_s, step_times, self._args.log_raw_data)
         self._result.add_raw_data(metric_t, throughput, self._args.log_raw_data)
 
@@ -420,11 +422,10 @@ def __process_model_result(self, model_action, precision, step_times):
                 return None
             if self._local_rank is None or self._global_rank == 0:
                 self._result.add_result(metric_s, statistics.mean(step_times))
-                throughput = [millisecond_per_second / step_time * self._args.batch_size for step_time in step_times]
-                self._result.add_result(metric_t, statistics.mean(throughput))
+                self._result.add_result(metric_t, mean_throughput)
         elif model_action == ModelAction.INFERENCE:
             self._result.add_result(metric_s, statistics.mean(step_times))
-            self._result.add_result(metric_t, statistics.mean(throughput))
+            self._result.add_result(metric_t, mean_throughput)
             self._process_percentile_result(metric_s, step_times)
             self._process_percentile_result(metric_t, throughput)
 
diff --git a/superbench/benchmarks/model_benchmarks/pytorch_bert.py b/superbench/benchmarks/model_benchmarks/pytorch_bert.py
index 86e2d31..a33b659 100644
--- a/superbench/benchmarks/model_benchmarks/pytorch_bert.py
+++ b/superbench/benchmarks/model_benchmarks/pytorch_bert.py
@@ -235,6 +235,10 @@ def _train_step(self, precision):
                 loss = self._loss_fn(output, self._target)
                 loss.backward()
                 self._optimizer.step()
+
+                if curr_step + 1 == self._args.num_warmup + self._args.num_steps:
+                    loss.item()
+
                 end = self._timer()
                 curr_step += 1
                 if curr_step > self._args.num_warmup:
@@ -265,9 +269,13 @@ def _inference_step(self, precision):
                         sample = sample.cuda()
                     if self._fp8_recipe is not None:
                         with te.fp8_autocast(enabled=True, fp8_recipe=self._fp8_recipe):
-                            self._model(sample)
+                            output = self._model(sample)
                     else:
-                        self._model(sample)
+                        output = self._model(sample)
+
+                    if curr_step + 1 == self._args.num_warmup + self._args.num_steps:
+                        output.item()
+
                     end = self._timer()
                     curr_step += 1
                     if curr_step > self._args.num_warmup:
diff --git a/superbench/benchmarks/model_benchmarks/pytorch_cnn.py b/superbench/benchmarks/model_benchmarks/pytorch_cnn.py
index ec947f0..cc4ef0a 100644
--- a/superbench/benchmarks/model_benchmarks/pytorch_cnn.py
+++ b/superbench/benchmarks/model_benchmarks/pytorch_cnn.py
@@ -6,6 +6,8 @@
 import torch
 from torchvision import models
 
+torch.moreh.options.miopen_mode=3
+
 from superbench.common.utils import logger
 from superbench.benchmarks import BenchmarkRegistry, Precision
 from superbench.benchmarks.model_benchmarks.model_base import Optimizer
@@ -109,6 +111,10 @@ def _train_step(self, precision):
                 loss = self._loss_fn(output, self._target)
                 loss.backward()
                 self._optimizer.step()
+
+                if curr_step + 1 == self._args.num_warmup + self._args.num_steps:
+                    loss.item()
+
                 end = self._timer()
                 curr_step += 1
                 if curr_step > self._args.num_warmup:
@@ -138,7 +144,11 @@ def _inference_step(self, precision):
                     start = self._timer()
                     if self._gpu_available:
                         sample = sample.cuda()
-                    self._model(sample)
+                    output = self._model(sample)
+
+                    if curr_step + 1 == self._args.num_warmup + self._args.num_steps:
+                        output.item()
+
                     end = self._timer()
                     curr_step += 1
                     if curr_step > self._args.num_warmup:
diff --git a/superbench/benchmarks/model_benchmarks/pytorch_gpt2.py b/superbench/benchmarks/model_benchmarks/pytorch_gpt2.py
index 5c46a1e..c15ff24 100644
--- a/superbench/benchmarks/model_benchmarks/pytorch_gpt2.py
+++ b/superbench/benchmarks/model_benchmarks/pytorch_gpt2.py
@@ -140,6 +140,10 @@ def _train_step(self, precision):
                 loss = self._loss_fn(output[range(self._args.batch_size), -1], self._target)
                 loss.backward()
                 self._optimizer.step()
+
+                if curr_step + 1 == self._args.num_warmup + self._args.num_steps:
+                    output.item()
+
                 end = self._timer()
                 curr_step += 1
                 if curr_step > self._args.num_warmup:
@@ -168,7 +172,11 @@ def _inference_step(self, precision):
                     start = self._timer()
                     if self._gpu_available:
                         sample = sample.cuda()
-                    self._model(sample)
+                    output = self._model(sample)
+
+                    if curr_step + 1 == self._args.num_warmup + self._args.num_steps:
+                        output.item()
+
                     end = self._timer()
                     curr_step += 1
                     if curr_step > self._args.num_warmup:
diff --git a/superbench/benchmarks/model_benchmarks/pytorch_lstm.py b/superbench/benchmarks/model_benchmarks/pytorch_lstm.py
index 0caa178..5de3a0b 100644
--- a/superbench/benchmarks/model_benchmarks/pytorch_lstm.py
+++ b/superbench/benchmarks/model_benchmarks/pytorch_lstm.py
@@ -5,6 +5,8 @@
 
 import torch
 
+torch.moreh.options.miopen_mode=3
+
 from superbench.common.utils import logger
 from superbench.benchmarks import BenchmarkRegistry, Precision
 from superbench.benchmarks.model_benchmarks.model_base import Optimizer
@@ -149,6 +151,10 @@ def _train_step(self, precision):
                 loss = self._loss_fn(output, self._target)
                 loss.backward()
                 self._optimizer.step()
+
+                if curr_step + 1 == self._args.num_warmup + self._args.num_steps:
+                    loss.item()
+
                 end = self._timer()
                 curr_step += 1
                 if curr_step > self._args.num_warmup:
@@ -178,8 +184,13 @@ def _inference_step(self, precision):
                     start = self._timer()
                     if self._gpu_available:
                         sample = sample.cuda()
-                    self._model(sample)
+                    output = self._model(sample)
+
+                    if curr_step + 1 == self._args.num_warmup + self._args.num_steps:
+                        output.item()
+
                     end = self._timer()
+
                     curr_step += 1
                     if curr_step > self._args.num_warmup:
                         # Save the step time of every training/inference step, unit is millisecond.
